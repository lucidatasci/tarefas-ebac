{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a447f72c-44bf-4e4c-b15f-86a062fb82f7",
   "metadata": {},
   "source": [
    "# Exercício 2 - Módulo 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915a015-aaf1-4e78-9a40-10926ee2e5e9",
   "metadata": {},
   "source": [
    "#### 1) Cite 5 diferenças entre o AdaBoost e o GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4599d414-a90f-420b-a124-2c9db1e029a4",
   "metadata": {},
   "source": [
    "| AdaBoost | GBM |\n",
    "|----------|----------|\n",
    "| Floresta feita de stumps | Floresta é feita de árvores completas |\n",
    "| Primeiro passo é um stump | Primeiro passo é a media do Y |\n",
    "| Cada resposta tem um peso diferente | Todas respostas têm um multiplicador em comum (learning_rate/eta) |\n",
    "| É mais sensível ao ruído e outliers nos dados | É menos sensível ao ruído devido ao uso de gradient descent |\n",
    "| Precisa de mais iterações para atingir um bom desempenho | Pode alcançar um bom desempenho com menos iterações |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18380f0-1386-41aa-b595-1cd5886f1330",
   "metadata": {},
   "source": [
    "#### 2) Acesse o link Scikit-learn–GBM, leia a explicação (traduza se for preciso) e crie um jupyternotebook contendo o exemplo de classificação e de regressão do GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a6c6a-e9c4-4efd-aa68-27ad89dc68c3",
   "metadata": {},
   "source": [
    "##### Classificação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2400382d-a90d-41dd-9400-457900aa47d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a42485-a41f-41bf-8771-cbafd336dabd",
   "metadata": {},
   "source": [
    "##### Regressão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde56610-b839-45e0-8161-7b611d67c9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n",
    "    loss='squared_error'\n",
    ").fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d30bf-5e35-498b-ad21-da771c49b073",
   "metadata": {},
   "source": [
    "#### 3) Cite 5 Hyperparametros importantes no GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e48ead-5399-44ad-a871-cdf2572f45d0",
   "metadata": {},
   "source": [
    "Válidos tanto para <i>GradientBoostingClassifier</i> quanto para <i>GradientBoostingRegressor</i>:\n",
    "\n",
    "<ul>\n",
    "  <li><b>n_estimators :</b> int, default=100 </li>\n",
    "  <li><b>learning_rate :</b> float, default=0.1 </li>\n",
    "  <li><b>subsample :</b> float, default=1.0 </li>\n",
    "  <li><b>criterion :</b> {‘friedman_mse’, ‘squared_error’}, default=’friedman_mse’</li>  \n",
    "  <li><b>min_samples_split :</b> int or float, default=2 </li>  \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5258fe-8f80-400d-b4ad-d76b07b595c3",
   "metadata": {},
   "source": [
    "#### 4) Utilize o GridSearchpara encontrar os melhores hyperparametrospara o conjunto de dados do exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c89685b-5563-4807-be5b-5f12954eea80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros:\n",
      "{'learning_rate': 1.0, 'max_depth': 1, 'n_estimators': 150}\n",
      "Melhor Score:\n",
      "0.9279999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "# Hiperparâmetros a ajustar\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.5, 1.0],\n",
    "    'max_depth': [1, 3, 5]\n",
    "}\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Melhores hiperparâmetros \n",
    "best_params = grid_search.best_params_\n",
    "print(\"Melhores hiperparâmetros:\")\n",
    "print(best_params)\n",
    "\n",
    "# Melhor Score médio\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Melhor Score:\")\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4978e6c2-6643-4f68-8974-05a6a7354fdf",
   "metadata": {},
   "source": [
    "#### 5) Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao StochasticGBM, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f94804-6a61-4f64-9b74-8f3240d94191",
   "metadata": {},
   "source": [
    "O GBM utiliza todas as observações disponíveis para ajustar o modelo a cada iteração, enquanto o StochasticGBM utiliza uma sub-amostra aleatória (estocástica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200306b6-bcf2-4147-859a-30d2acb93e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
